[{"average_distance":0.3336116910229645,"minimum_distance":0.22546972860125258,"question":"What does the `isinplace` option control, and what happens if parameters are not provided?","source":"https://docs.sciml.ai/LinearSolve/dev/basics/LinearProblem - Linear Problems","distance_scores":[0.3423799582463466,0.3423799582463466,0.5323590814196242,0.22546972860125258,0.22546972860125258],"context":"`isinplace` optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for `AbstractMatrix` , and for `AbstractSciMLOperator` s it matches the choice of the operator definition. Parameters are optional, and if not given, then a `NullParameters()` singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. Fields","answer":"The `isinplace` option controls whether the function operates in-place, meaning whether the solvers are allowed to mutate. By default, this option is set to true for `AbstractMatrix`, and it aligns with the operator definition for `AbstractSciMLOperator`. If parameters are not provided, a `NullParameters()` singleton will be used, which will throw errors if non-existent parameters are indexed.","retrieved_contexts":["1. ```julia OptimizationProblem{iip}(f, u0, p = SciMLBase.NullParameters(),;\n                        lb = nothing,\n                        ub = nothing,\n                        lcons = nothing,\n                        ucons = nothing,\n                        sense = nothing,\n                        kwargs...)```\n`isinplace` optionally sets whether the function is in-place or not. This is determined automatically, but not inferred. Note that for OptimizationProblem, in-place refers to the objective's derivative functions, the constraint function and its derivatives. `OptimizationProblem` currently only supports in-place.\nParameters `p` are optional, and if not given, then a `NullParameters()` singleton will be used, which will throw nice errors if you try to index non-existent parameters.","2. ```julia OptimizationProblem{iip}(f, u0, p = SciMLBase.NullParameters(),;\n                        lb = nothing,\n                        ub = nothing,\n                        lcons = nothing,\n                        ucons = nothing,\n                        sense = nothing,\n                        kwargs...)```\n`isinplace` optionally sets whether the function is in-place or not. This is determined automatically, but not inferred. Note that for OptimizationProblem, in-place refers to the objective's derivative functions, the constraint function and its derivatives. `OptimizationProblem` currently only supports in-place.\nParameters `p` are optional, and if not given, then a `NullParameters()` singleton will be used, which will throw nice errors if you try to index non-existent parameters.","3. `DiscreteProblem{isinplace,specialize}(f,u0,tspan,p=NullParameters();kwargs...)` : Defines the discrete problem with the specified functions. `DiscreteProblem{isinplace,specialize}(u0,tspan,p=NullParameters();kwargs...)` : Defines the discrete problem with the identity map.\n`isinplace` optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. `specialize` optionally controls the specialization level. See the  specialization levels section of the SciMLBase documentation for more details. The default is `AutoSpecialize` .\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.","4. ```julia SteadyStateProblem(f::ODEFunction, u0, p = NullParameters(); kwargs...)\nSteadyStateProblem{isinplace, specialize}(f, u0, p = NullParameters(); kwargs...)```\n`isinplace` optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. `specialize` optionally controls the specialization level. See the  specialization levels section of the SciMLBase documentation for more details. The default is `AutoSpecialize` .\nParameters are optional, and if not given, a `NullParameters()` singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a `callback` in the problem, then that `callback` will be added in every solve call.","5. ```julia SteadyStateProblem(f::ODEFunction, u0, p = NullParameters(); kwargs...)\nSteadyStateProblem{isinplace, specialize}(f, u0, p = NullParameters(); kwargs...)```\n`isinplace` optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. `specialize` optionally controls the specialization level. See the  specialization levels section of the SciMLBase documentation for more details. The default is `AutoSpecialize` .\nParameters are optional, and if not given, a `NullParameters()` singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a `callback` in the problem, then that `callback` will be added in every solve call."]},{"average_distance":0.2810810810810811,"minimum_distance":0.0,"question":"What is Catalyst’s core functionality and what types of simulations can the models be used for?","source":"https://docs.sciml.ai/Catalyst/stable/model_simulation/simulation_introduction - Model Simulation Introduction","distance_scores":[0.0,0.4756756756756757,0.4648648648648649,0.4648648648648649,0.0],"context":"Catalyst's core functionality is the creation of  chemical reaction network (CRN) models that can be simulated using ODE, SDE, and jump simulations. How such simulations are carried out has already been described in  Catalyst's introduction . This page provides a deeper introduction, giving some additional background and introducing various simulation-related options.","answer":"Catalyst's core functionality is the creation of chemical reaction network (CRN) models that can be simulated using ODE, SDE, and jump simulations.","retrieved_contexts":["1. Catalyst's core functionality is the creation of  chemical reaction network (CRN) models that can be simulated using ODE, SDE, and jump simulations. How such simulations are carried out has already been described in  Catalyst's introduction . This page provides a deeper introduction, giving some additional background and introducing various simulation-related options.\nHere we will focus on the basics, with other sections of the simulation documentation describing various specialised features, or giving advice on performance. Anyone who plans on using Catalyst's simulation functionality extensively is recommended to also read the documentation on  solution plotting , and on how to  interact with simulation problems, integrators, and solutions .","2. While Catalyst has primarily been designed around the modelling of biological systems, reaction network models are also common in chemistry. This section describes two types of functionality, that while of general interest, should be especially useful in the modelling of chemical systems.\nThe `@compound` option, which enables the user to designate that a specific species is composed of certain subspecies. The `balance_reaction` function, which enables the user to balance a reaction so the same number of components occur on both sides.","3. Catalyst is based around the creation, analysis, and simulation of chemical reaction network models. Catalyst stores these models in  `ReactionSystem structures. This page describes some basic functions for accessing the content of these structures. This includes retrieving lists of species, parameters, or reactions that a model consists of.\nAn extensive list of relevant functions for working with `ReactionSystem` models can be found in Catalyst's  API .","4. The Catalyst tool for the modelling of chemical reaction networks is based in the Julia programming language   [1]   [2] . While experience in Julia programming is advantageous for using Catalyst, it is not necessary for accessing most of its basic features.\nThis tutorial serves as an introduction to Catalyst for those unfamiliar with Julia, while also introducing some basic Julia concepts. Anyone who plans on using Catalyst extensively is recommended to familiarise oneself more thoroughly with the Julia programming language.","5. Catalyst's core functionality is the creation of  chemical reaction network (CRN) models that can be simulated using ODE, SDE, and jump simulations. How such simulations are carried out has already been described in  Catalyst's introduction . This page provides a deeper introduction, giving some additional background and introducing various simulation-related options.\nHere we will focus on the basics, with other sections of the simulation documentation describing various specialised features, or giving advice on performance. Anyone who plans on using Catalyst's simulation functionality extensively is recommended to also read the documentation on  solution plotting , and on how to  interact with simulation problems, integrators, and solutions .\nAnyone with an application for which performance is critical should consider reading the corresponding page on performance advice for  ODEs or  SDEs ."]},{"average_distance":0.25479452054794516,"minimum_distance":0.002739726027397249,"question":"What is the purpose of the `NLsolveJL` package and what submethod choices does it offer?","source":"https://docs.sciml.ai/NonlinearSolve/stable/solvers/nonlinear_system_solvers - Nonlinear System Solvers/Full List of Methods/NLsolve.jl","distance_scores":[0.002739726027397249,0.002739726027397249,0.4657534246575342,0.410958904109589,0.39178082191780816],"context":"This is a wrapper package for importing solvers from NLsolve.jl into the SciML interface. `NLsolveJL()` : A wrapper for  NLsolve.jl Submethod choices for this algorithm include: `:anderson` : Anderson-accelerated fixed-point iteration `:newton` : Classical Newton method with an optional line search `:trust_region` : Trust region Newton method (the default choice)","answer":"The `NLsolveJL` package is a wrapper package for importing solvers from NLsolve.jl into the SciML interface. The submethods offered for this algorithm include: `:anderson`, which is an Anderson-accelerated fixed-point iteration; `:newton`, which is the classical Newton method with an optional line search; and `:trust_region`, which is the trust region Newton method and is the default choice.","retrieved_contexts":["1. This is a wrapper package for importing solvers from NLsolve.jl into the SciML interface. `NLsolveJL()` : A wrapper for  NLsolve.jl Submethod choices for this algorithm include: `:anderson` : Anderson-accelerated fixed-point iteration `:newton` : Classical Newton method with an optional line search\n`:trust_region` : Trust region Newton method (the default choice)","2. This is a wrapper package for importing solvers from NLsolve.jl into the SciML interface. `NLsolveJL()` : A wrapper for  NLsolve.jl Submethod choices for this algorithm include: `:anderson` : Anderson-accelerated fixed-point iteration `:newton` : Classical Newton method with an optional line search\n`:trust_region` : Trust region Newton method (the default choice)","3. ```julia NLSolversJL(method; autodiff = nothing)\nNLSolversJL(; method, autodiff = nothing)``` Wrapper over NLSolvers.jl Nonlinear Equation Solvers. We automatically construct the jacobian function and supply it to the solver. Arguments `method` : the choice of method for solving the nonlinear system. See the documentation for NLSolvers.jl for more information.\n`autodiff` : the choice of method for generating the Jacobian. Defaults to `nothing` which means that a default is selected according to the problem specification. Can be any valid ADTypes.jl autodiff type (conditional on that backend being supported in NonlinearSolve.jl). source","4. These benchmarks compares the runtime and error for a range of nonlinear solvers. The solvers are implemented in  NonlinearProblemLibrary.jl , where you can find the problem function declarations. We test the following solvers: NonlinearSolve.jl's  Newton Raphson method ( `NewtonRaphson()` ). NonlinearSolve.jl's  Newton trust region method ( `TrustRegion()` ).\nNonlinearSolve.jl's Levenberg-Marquardt method ( `LevenbergMarquardt()` ). NonlinearSolve.jl's Broyden method ( `Broyden()` ). MINPACK's  Modified Powell method ( `CMINPACK(method=:hybr)` ). MINPACK's  Levenberg-Marquardt method ( `CMINPACK(method=:lm)` ). NLsolveJL's  Newton Raphson ( `NLsolveJL(method=:newton)` ).\nNLsolveJL's  Newton trust region ( `NLsolveJL()` ). NLsolveJL's  Anderson acceleration ( `NLsolveJL(method=:anderson)` ). Sundials's  Newton-Krylov method ( `KINSOL()` ).","5. NonlinearSolve.jl is a unified interface for the nonlinear solving packages of Julia. The package includes its own high-performance nonlinear solvers which include the ability to swap out to fast direct and iterative linear solvers, along with the ability to use sparse automatic differentiation for Jacobian construction and Jacobian-vector products.\nNonlinearSolve.jl interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the  ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.\nPerformance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue. Consult the  NonlinearSystemSolvers page for information on how to import solvers from different packages."]},{"average_distance":0.22867647058823531,"minimum_distance":0.0,"question":"What is the significance of the `differ_weight` kwarg in the `build_loss_objective` function using L2Loss?","source":"https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/#Alternative-Cost-Functions-for-Increased-Robustness - Getting Started with Optimization-Based ODE Parameter Estimation/Estimating Multiple Parameters Simultaneously/Alternative Cost Functions for Increased Robustness","distance_scores":[0.0,0.0,0.3088235294117647,0.3602941176470589,0.4742647058823529],"context":"The `build_loss_objective` with `L2Loss` is the most naive approach for parameter estimation. There are many others. We can also use First-Differences in L2Loss by passing the kwarg `differ_weight` which decides the contribution of the differencing loss to the total loss.","answer":"The `differ_weight` kwarg in the `build_loss_objective` function using L2Loss is used to decide the contribution of the differencing loss to the total loss.","retrieved_contexts":["1. The `build_loss_objective` with `L2Loss` is the most naive approach for parameter estimation. There are many others. We can also use First-Differences in L2Loss by passing the kwarg `differ_weight` which decides the contribution of the differencing loss to the total loss.\n```julia cost_function = build_loss_objective(prob, Tsit5(),\n                                     L2Loss(t, data, differ_weight = 0.3,\n                                            data_weight = 0.7),\n                                     Optimization.AutoForwardDiff(),\n                                     maxiters = 10000, verbose = false)","2. The `build_loss_objective` with `L2Loss` is the most naive approach for parameter estimation. There are many others. We can also use First-Differences in L2Loss by passing the kwarg `differ_weight` which decides the contribution of the differencing loss to the total loss.\n```julia cost_function = build_loss_objective(prob, Tsit5(),\n                                     L2Loss(t, data, differ_weight = 0.3,\n                                            data_weight = 0.7),\n                                     Optimization.AutoForwardDiff(),\n                                     maxiters = 10000, verbose = false)","3. where `t` is the set of timepoints which the data are found at, and `data` are the values that are known where each column corresponds to measures of the values of the system. `L2Loss` is an optimized version of the L2-distance. The `data_weight` is a scalar or vector of weights for the loss function which must match the size of the data.\nNote that minimization of a weighted `L2Loss` is equivalent to maximum likelihood estimation of a heteroskedastic Normally distributed likelihood. `differ_weight` allows one to add a weight on the first differencing terms `sol[i+1]-sol[i]` against the data first differences.\nThis smooths out the loss term and can make it easier to fit strong solutions of stochastic models, but is zero (nothing) by default. Additionally, `colloc_grad` allows one to give a matrix of the collocation gradients for the data. This is used to add an interpolation derivative term, like the two-stage method.","4. ```julia L2Loss(t, data, differ_weight = 0.3, data_weight = 0.7)```\nFirst differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the `L2Loss` alone leads to non-identifiable parameters, but adding a first differencing term makes it more identifiable.\nThis can be noted on stochastic differential equation models, where this aims to capture the autocorrelation and therefore helps us avoid getting the same stationary distribution despite different trajectories and thus wrong parameter estimates.","5. ```julia L2Loss(t, data, differ_weight = 0.3, data_weight = 0.7)```\nFirst differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the `L2Loss` alone leads to non-identifiable parameters, but adding a first differencing term makes it more identifiable."]},{"average_distance":0.006117647058823494,"minimum_distance":0.0023529411764705577,"question":"What are the advantages of avoiding closures in programming?","source":"https://docs.sciml.ai/SciMLStyle/stable/ - SciML Style Guide for Julia/Overarching Dogmas of the SciML Style/Closures should be avoided whenever possible","distance_scores":[0.0023529411764705577,0.0023529411764705577,0.0117647058823529,0.0117647058823529,0.0023529411764705577],"context":"Closures can cause accidental type instabilities that are difficult to track down and debug; in the long run it saves time to always program defensively and avoid writing closures in the first place, even when a particular closure would not have been problematic. A similar argument applies to reading code with closures; if someone is looking for type instabilities, this is faster to do when code does not contain closures.","answer":"Avoiding closures in programming helps prevent accidental type instabilities that are difficult to track down and debug. Additionally, it makes reading code faster when checking for type instabilities, as the absence of closures simplifies this process.","retrieved_contexts":["1. Closures can cause accidental type instabilities that are difficult to track down and debug; in the long run it saves time to always program defensively and avoid writing closures in the first place, even when a particular closure would not have been problematic.\nA similar argument applies to reading code with closures; if someone is looking for type instabilities, this is faster to do when code does not contain closures. Furthermore, if you want to update variables in an outer scope, do so explicitly with `Ref` s or self defined structs. For example,","2. Closures can cause accidental type instabilities that are difficult to track down and debug; in the long run it saves time to always program defensively and avoid writing closures in the first place, even when a particular closure would not have been problematic.\nA similar argument applies to reading code with closures; if someone is looking for type instabilities, this is faster to do when code does not contain closures. Furthermore, if you want to update variables in an outer scope, do so explicitly with `Ref` s or self defined structs. For example,","3. Closures can cause accidental type instabilities that are difficult to track down and debug; in the\nlong run, it saves time to always program defensively and avoid writing closures in the first place,\neven when a particular closure would not have been problematic.\nA similar argument applies to reading\ncode with closures; if someone is looking for type instabilities, this is faster to do when code does\nnot contain closures.\nFurthermore, if you want to update variables in an outer scope, do so explicitly with `Ref` s or self\ndefined structs.\nFor example,","4. Closures can cause accidental type instabilities that are difficult to track down and debug; in the\nlong run, it saves time to always program defensively and avoid writing closures in the first place,\neven when a particular closure would not have been problematic.\nA similar argument applies to reading\ncode with closures; if someone is looking for type instabilities, this is faster to do when code does\nnot contain closures.\nFurthermore, if you want to update variables in an outer scope, do so explicitly with `Ref` s or self\ndefined structs.\nFor example,\nmap (Base  .  Fix2 (getindex, i), vector_of_vectors) is preferred over map (v  -> v[i], vector_of_vectors) or [v[i]  for v  in vector_of_vectors]","5. Closures can cause accidental type instabilities that are difficult to track down and debug; in the long run it saves time to always program defensively and avoid writing closures in the first place, even when a particular closure would not have been problematic.\nA similar argument applies to reading code with closures; if someone is looking for type instabilities, this is faster to do when code does not contain closures. Furthermore, if you want to update variables in an outer scope, do so explicitly with `Ref` s or self defined structs. For example,\n```julia map(Base.Fix2(getindex, i), vector_of_vectors)``` is preferred over ```julia map(v -> v[i], vector_of_vectors)``` or ```julia [v[i] for v in vector_of_vectors]```"]},{"average_distance":0.46743883394065594,"minimum_distance":0.34558823529411764,"question":"What is the purpose of the ThermalCollector model and how does it operate?","source":"https://docs.sciml.ai/ModelingToolkitStandardLibrary/stable/API/thermal/ - ModelingToolkitStandardLibrary: Thermal Components/Thermal Components","distance_scores":[0.5575221238938053,0.34558823529411764,0.34558823529411764,0.5442477876106195,0.5442477876106195],"context":"Lumped thermal element transporting heat without storing it. States: `dT` :  [ `K` ] Temperature difference across the component a.T - b.T `Q_flow` : [ `W` ] Heat flow rate from port a -> port b Connectors: `port_a` `port_b` Parameters: `R` : [ `K/W` ] Constant thermal resistance of material source ```julia ThermalCollector(; name, m = 1)``` Collects `m` heat flows This is a model to collect the heat flows from `m` heatports to one single heatport.","answer":"The purpose of the ThermalCollector model is to collect the heat flows from multiple heatports and combine them into one single heatport. It operates by taking inputs from `m` heat flows and conveying them to one centralized heatport.","retrieved_contexts":["1. The following example estimates the amount of thermal power transferred from a solar collector embedded in a concrete floor, to a water reservoir. The power is computed by measuring the temperature difference,  $\\Delta T$ , between the solar collectors circulating warm water going into the collector tank and the colder returning water.\nUsing the mass-flow rate and the specific heat capacity of water, we can estimate the power transfer. No flow meter is installed, so the flow is estimated and subject to large uncertainty.","2. ModelingToolkitStandardLibrary: Thermal Components Index   Thermal Utilities   Thermal Components   Thermal Sensors   Thermal Sources","3. ModelingToolkitStandardLibrary: Thermal Components Index   Thermal Utilities   Thermal Components   Thermal Sensors   Thermal Sources","4. `ModelingToolkitStandardLibrary.Thermal.BodyRadiation` `ModelingToolkitStandardLibrary.Thermal.ConvectiveConductor` `ModelingToolkitStandardLibrary.Thermal.ConvectiveResistor` `ModelingToolkitStandardLibrary.Thermal.Element1D` `ModelingToolkitStandardLibrary.Thermal.FixedHeatFlow` `ModelingToolkitStandardLibrary.Thermal.FixedTemperature`\n`ModelingToolkitStandardLibrary.Thermal.HeatCapacitor` `ModelingToolkitStandardLibrary.Thermal.HeatFlowSensor` `ModelingToolkitStandardLibrary.Thermal.HeatPort` `ModelingToolkitStandardLibrary.Thermal.PrescribedHeatFlow` `ModelingToolkitStandardLibrary.Thermal.PrescribedTemperature` `ModelingToolkitStandardLibrary.Thermal.RelativeTemperatureSensor`\n`ModelingToolkitStandardLibrary.Thermal.TemperatureSensor` `ModelingToolkitStandardLibrary.Thermal.ThermalConductor` `ModelingToolkitStandardLibrary.Thermal.ThermalResistor` `ModelingToolkitStandardLibrary.Thermal.ThermalCollector`","5. `ModelingToolkitStandardLibrary.Thermal.BodyRadiation` `ModelingToolkitStandardLibrary.Thermal.ConvectiveConductor` `ModelingToolkitStandardLibrary.Thermal.ConvectiveResistor` `ModelingToolkitStandardLibrary.Thermal.Element1D` `ModelingToolkitStandardLibrary.Thermal.FixedHeatFlow` `ModelingToolkitStandardLibrary.Thermal.FixedTemperature`\n`ModelingToolkitStandardLibrary.Thermal.HeatCapacitor` `ModelingToolkitStandardLibrary.Thermal.HeatFlowSensor` `ModelingToolkitStandardLibrary.Thermal.HeatPort` `ModelingToolkitStandardLibrary.Thermal.PrescribedHeatFlow` `ModelingToolkitStandardLibrary.Thermal.PrescribedTemperature` `ModelingToolkitStandardLibrary.Thermal.RelativeTemperatureSensor`\n`ModelingToolkitStandardLibrary.Thermal.TemperatureSensor` `ModelingToolkitStandardLibrary.Thermal.ThermalConductor` `ModelingToolkitStandardLibrary.Thermal.ThermalResistor` `ModelingToolkitStandardLibrary.Thermal.ThermalCollector`"]},{"average_distance":0.21325623045867098,"minimum_distance":0.002109704641350185,"question":"What is the `colorvec` in the context of Jacobian construction and its default behavior?","source":"https://docs.sciml.ai/DiffEqDocs/dev/types/split_ode_types - Split ODE Problems","distance_scores":[0.5299760191846523,0.5299760191846523,0.002109704641350185,0.002109704641350185,0.002109704641350185],"context":"`colorvec` : a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the `jac_prototype` . This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to `nothing` , which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.","answer":"The `colorvec` is a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the `jac_prototype`. It specializes the Jacobian construction for accelerated computation using finite differences and automatic differentiation based on the sparsity pattern. By default, `colorvec` is set to `nothing`, meaning a color vector is computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.","retrieved_contexts":["1. colorvec vectors are allowed to be supplied to the Jacobian routines, and these are the directional derivatives for constructing the Jacobian. For example, an accurate NxN tridiagonal Jacobian can be computed in just 4 `f` calls by using `colorvec=repeat(1:3,N÷3)` . For information on automatically generating colorvec vectors of sparse matrices, see  SparseDiffTools.jl .\nHessian coloring support is coming soon!","2. colorvec vectors are allowed to be supplied to the Jacobian routines, and these are the directional derivatives for constructing the Jacobian. For example, an accurate NxN tridiagonal Jacobian can be computed in just 4 `f` calls by using `colorvec=repeat(1:3,N÷3)` . For information on automatically generating colorvec vectors of sparse matrices, see  SparseDiffTools.jl .\nHessian coloring support is coming soon!","3. `paramjac(pJ,u,p,t)` : returns the parameter Jacobian  $\\frac{df}{dp}$ .\n`colorvec` : a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the `jac_prototype` . This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern.\nDefaults to `nothing` , which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.","4. `paramjac(pJ,u,p,t)` : returns the parameter Jacobian  $\\frac{df}{dp}$ .\n`colorvec` : a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the `jac_prototype` . This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern.\nDefaults to `nothing` , which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.","5. `paramjac(pJ,h,u,p,t)` : returns the parameter Jacobian  $\\frac{df}{dp}$ .\n`colorvec` : a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the `jac_prototype` . This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern.\nDefaults to `nothing` , which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern."]},{"average_distance":0.37550634547636413,"minimum_distance":0.0,"question":"What packages extend the functionality of AbstractFFTs and what do they provide?","source":"https://docs.sciml.ai/AbstractFFTs/stable/implementations - FFT Implementations/Existing packages","distance_scores":[0.0,0.5,0.4948979591836735,0.5182926829268293,0.3643410852713178],"context":"The following packages extend the functionality provided by AbstractFFTs: FFTW.jl : Bindings for the  FFTW library. This also used to be part of Base Julia. FastTransforms.jl : Pure-Julia implementation of FFT, with support for arbitrary AbstractFloat types.","answer":"The packages that extend the functionality of AbstractFFTs are FFTW.jl and FastTransforms.jl. FFTW.jl provides bindings for the FFTW library and was also part of Base Julia previously. FastTransforms.jl offers a pure-Julia implementation of FFT with support for arbitrary AbstractFloat types.","retrieved_contexts":["1. The following packages extend the functionality provided by AbstractFFTs: FFTW.jl : Bindings for the  FFTW library. This also used to be part of Base Julia. FastTransforms.jl : Pure-Julia implementation of FFT, with support for arbitrary AbstractFloat types.","2. AbstractFFTs.jl defines the common interface for Fast Fourier Transformations (FFTs) in Julia. Similar to SciMLBase.jl, AbstractFFTs.jl is not a solver library but instead a shared API which is extended by solver libraries such as  FFTW.jl . Code written using AbstractFFTs.jl can be made compatible with FFT libraries without having an explicit dependency on a solver.","3. This package provides a generic framework for defining  fast Fourier transform (FFT) implementations in Julia. The code herein was part of Julia's Base library for Julia versions 0.6 and lower.","4. Public Interface FFT and FFT planning functions Adjoint functionality FFT Implementations Existing packages Defining a new implementation Testing implementations","5. `AbstractFFTs.jl` provides an experimental `TestUtils` module to help with testing downstream implementations, available as a  weak extension of `Test` . The following functions test that all FFT functionality has been correctly implemented: ```julia TestUtils.test_complex_ffts(ArrayType=Array; test_inplace=true, test_adjoint=true)```\nRun tests to verify correctness of FFT, BFFT, and IFFT functionality using a particular backend plan implementation.  The backend implementation is assumed to be loaded prior to calling this function. Arguments `ArrayType` : determines the `AbstractArray` implementation for which the correctness tests are run. Arrays are constructed via `convert(ArrayType, ...)` ."]},{"average_distance":0.39666666666666667,"minimum_distance":0.0023529411764705577,"question":"How can the initial-value problem of the delayed system be implemented, and what parameters and conditions are used?","source":"https://turinglang.org/docs/tutorials/10-bayesian-differential-equations/index.html - Bayesian Estimation of Differential Equations/Inference of Delay Differential Equations","distance_scores":[0.0023529411764705577,0.46352941176470586,0.48705882352941177,0.48705882352941177,0.5433333333333333],"context":"The initial-value problem of the delayed system can be implemented as a  `DDEProblem` . As described in the  DDE example , here the function `h` is the history function that can be used to obtain a state at an earlier time point. Again we use parameters  \\(\\alpha = 1.5\\) ,  \\(\\beta = 1\\) ,  \\(\\gamma = 3\\) , and  \\(\\delta = 1\\) and initial conditions  \\(x(0) = y(0) = 1\\) . Moreover, we assume  \\(x(t) = 1\\) for  \\(t < 0\\) .","answer":"The initial-value problem of the delayed system can be implemented as a `DDEProblem`. The parameters used are α = 1.5, β = 1, γ = 3, and δ = 1. The initial conditions are x(0) = y(0) = 1. Additionally, x(t) = 1 for t < 0.","retrieved_contexts":["1. \\[\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} &= \\alpha x(t-\\tau) - \\beta y(t) x(t),\\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &= - \\gamma y(t) + \\delta x(t) y(t),\n\\end{aligned}\n\\] where  \\(\\tau\\) is a (positive) delay and  \\(x(t-\\tau)\\) is the variable  \\(x\\) at an earlier time point  \\(t - \\tau\\) .\nThe initial-value problem of the delayed system can be implemented as a  `DDEProblem` . As described in the  DDE example , here the function `h` is the history function that can be used to obtain a state at an earlier time point. Again we use parameters  \\(\\alpha = 1.5\\) ,  \\(\\beta = 1\\) ,  \\(\\gamma = 3\\) , and  \\(\\delta = 1\\) and initial conditions  \\(x(0) = y(0) = 1\\) .\nMoreover, we assume  \\(x(t) = 1\\) for  \\(t < 0\\) .","2. \\[f(\\int_{0}^{\\inf} X_0(t-\\tau)g(\\tau) d\\tau),\\hspace{0.33cm} ∅ \\to X\\] for some kernel  $g(\\tau)$ . Here, a common kernel is a  gamma distribution , which generates a gamma-distributed delay: \\[g(\\tau; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}\\tau^{\\alpha-1}}{\\Gamma(\\alpha)}e^{-\\beta\\tau}\\]\nWhen this is converted to an ODE, this generates an integro-differential equation. These (as well as the simpler delay differential equations) can be difficult to solve and analyse (especially when SDE or jump simulations are desired). Here,  the linear chain trick can be used to instead model the delay as a linear pathway of the form described above   [2] .\nA result by Fargue shows that this is equivalent to a gamma-distributed delay, where  $\\alpha$ is equivalent to  $n$ (the number of species in our linear pathway) and  $\\beta$ to %\\tau$ (the delay length term)   [3] .","3. Paul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England. source ```julia prob_dde_DDETST_C3``` Delay differential equation model of hematopoiesis, given by \\[u_1'(t) = \\hat{s}_0 u_2(t - T_1) - \\gamma u_1(t) - Q,\\]\n\\[u_2'(t) = f(u_1(t)) - k u_2(t),\\] \\[u_3'(t) = 1 - \\frac{Q \\exp(\\gamma u_3(t))}{\\hat{s}_0 u_2(t - T_1 - u_3(t))},\\] for  $t \\in [0, 300]$ with history function  $\\phi_1(0) = 3.325$ ,  $\\phi_3(0) = 120$ , and \\[\\phi_2(t) = \\begin{cases}\n  10 & \\text{if } t \\in [- T_1, 0],\\\\\n  9.5 & \\text{if } t < - T_1,\n\\end{cases}\\]\nwhere  $f(y) = a / (1 + K y^r)$ ,  $\\hat{s}_0 = 0.0031$ ,  $T_1 = 6$ ,  $\\gamma = 0.001$ ,  $Q = 0.0275$ ,  $k = 2.8$ ,  $a = 6570$ ,  $K = 0.0382$ , and  $r = 6.96$ . References Mahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication. source","4. Paul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England. source ```julia prob_dde_DDETST_C3``` Delay differential equation model of hematopoiesis, given by \\[u_1'(t) = \\hat{s}_0 u_2(t - T_1) - \\gamma u_1(t) - Q,\\]\n\\[u_2'(t) = f(u_1(t)) - k u_2(t),\\] \\[u_3'(t) = 1 - \\frac{Q \\exp(\\gamma u_3(t))}{\\hat{s}_0 u_2(t - T_1 - u_3(t))},\\] for  $t \\in [0, 300]$ with history function  $\\phi_1(0) = 3.325$ ,  $\\phi_3(0) = 120$ , and \\[\\phi_2(t) = \\begin{cases}\n  10 & \\text{if } t \\in [- T_1, 0],\\\\\n  9.5 & \\text{if } t < - T_1,\n\\end{cases}\\]\nwhere  $f(y) = a / (1 + K y^r)$ ,  $\\hat{s}_0 = 0.0031$ ,  $T_1 = 6$ ,  $\\gamma = 0.001$ ,  $Q = 0.0275$ ,  $k = 2.8$ ,  $a = 6570$ ,  $K = 0.0382$ , and  $r = 6.96$ . References Mahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication. source","5. We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques."]},{"average_distance":0.3692660550458716,"minimum_distance":0.002293577981651418,"question":"What is the integration status of Sundials.jl in the DiffEq ecosystem, and in which scenarios are its multistep schemes most efficient?","source":"https://sciml.ai/news/2017/12/11/Events/ - DifferentialEquations.jl 3.2: Expansion of Event Compatibility/Integrators, Callbacks, and Events with Sundials","distance_scores":[0.002293577981651418,0.45871559633027525,0.45871559633027525,0.45871559633027525,0.4678899082568807],"context":"Sundials.jl is now very well integrated into the DiffEq ecosystem. It's compatible with almost every option. The things it's not compatible with, such as arbitrary number types, do not seem to fit into the main purposes of its schemes anyways. The multistep schemes from this library are most efficient in cases where the user's `f` is expensive or the number of ODEs is large. In these cases, arbitrary precision is infeasible anyways.","answer":"Sundials.jl is very well integrated into the DiffEq ecosystem and compatible with almost every option. Its multistep schemes are most efficient when the user's `f` is expensive or the number of ODEs is large.","retrieved_contexts":["1. Note that not all of the integrator interface functions are implemented right now, but at this point implementing most of them is quite simple. So please feel free to open feature requests and we can tackle them as needed.\nSundials.jl is now very well integrated into the DiffEq ecosystem. It's compatible with almost every option. The things it's not compatible with, such as arbitrary number types, do not seem to fit into the main purposes of its schemes anyways. The multistep schemes from this library are most efficient in cases where the user's `f` is expensive or the number of ODEs is large.\nIn these cases, arbitrary precision is infeasible anyways. Thus, since Sundials is a very well-optimized library, I am putting the development of a native Julia multistep method on the backburner.","2. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl: ```julia using Pkg\nPkg.add(\"Sundials\")\nusing Sundials```\nThe Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and “generally works”.\n`CVODE_BDF` - CVode Backward Differentiation Formula (BDF) solver. `CVODE_Adams` - CVode Adams-Moulton solver. `ARKODE` - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options. The Sundials algorithms all come with a 3rd order Hermite polynomial interpolation.","3. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl: ```julia using Pkg\nPkg.add(\"Sundials\")\nusing Sundials```\nThe Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and “generally works”.\n`CVODE_BDF` - CVode Backward Differentiation Formula (BDF) solver. `CVODE_Adams` - CVode Adams-Moulton solver. `ARKODE` - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options. The Sundials algorithms all come with a 3rd order Hermite polynomial interpolation.","4. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl: ```julia using Pkg\nPkg.add(\"Sundials\")\nusing Sundials```\nThe Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and “generally works”.\n`CVODE_BDF` - CVode Backward Differentiation Formula (BDF) solver. `CVODE_Adams` - CVode Adams-Moulton solver. `ARKODE` - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options. The Sundials algorithms all come with a 3rd order Hermite polynomial interpolation.","5. At the higher order end of the spectrum, methods like `Kvaerno5` and `KenCarp4` have similar results in the slow changing Jacobian domain but are efficient at achieving high accuracy.\nThus, with the introduction of these SDIRK methods, we have been hard pressed to find problems where `CVODE_BDF` is not heavily surpassed by one of the OrdinaryDiffEq.jl methods. Sundials has now been relegated to the \"lowish accuracy huge PDEs\" niche. However, we will soon have an answer to that in the form of IMEX methods which will be explained below.\nIn addition, most of the wrapped Fortran methods don't have a niche at all. The one wrapped method that we see as really filling a niche that we do not hit is `radau` . If you need to solve stiff equations with error `<1e-7` , `radau` is still the method to go to."]},{"average_distance":0.15550935550935546,"minimum_distance":0.0020790020790020236,"question":"What is the purpose of the Backend Agnostic API provided by the `reduce!` function in Lux?","source":"https://lux.csail.mit.edu/dev/api/Lux/distributed_utils - Distributed Utils  ​/Communication Primitives  ​","distance_scores":[0.0935550935550935,0.0935550935550935,0.0020790020790020236,0.0020790020790020236,0.5862785862785862],"context":"```julia reduce! (backend :: AbstractLuxDistributedBackend , sendrecvbuf, op; root :: Int = 0 ) \n reduce! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf, op; root :: Int = 0 )``` Backend Agnostic API to perform a reduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf` . `op` allows a special `DistributedUtils.avg` operation that averages the result across all workers. source # Lux.DistributedUtils.synchronize!! — Function","answer":"The purpose of the Backend Agnostic API provided by the `reduce!` function in Lux is to perform a reduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf`. It supports a special `DistributedUtils.avg` operation to average the result across all workers.","retrieved_contexts":["1. # Lux.DistributedUtils.allreduce! — Function . julia ```julia allreduce! (backend :: AbstractLuxDistributedBackend , sendrecvbuf, op) \n allreduce! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf, op)``` Backend Agnostic API to perform an allreduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf` .\n`op` allows a special `DistributedUtils.avg` operation that averages the result across all workers. source # Lux.DistributedUtils.bcast! — Function . julia ```julia bcast! (backend :: AbstractLuxDistributedBackend , sendrecvbuf; root :: Int = 0 ) \n bcast! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf; root :: Int = 0 )```","2. # Lux.DistributedUtils.allreduce! — Function . julia ```julia allreduce! (backend :: AbstractLuxDistributedBackend , sendrecvbuf, op) \n allreduce! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf, op)``` Backend Agnostic API to perform an allreduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf` .\n`op` allows a special `DistributedUtils.avg` operation that averages the result across all workers. source # Lux.DistributedUtils.bcast! — Function . julia ```julia bcast! (backend :: AbstractLuxDistributedBackend , sendrecvbuf; root :: Int = 0 ) \n bcast! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf; root :: Int = 0 )```","3. Backend Agnostic API to broadcast the given buffer `sendrecvbuf` or `sendbuf` to all workers into `recvbuf` . The value at `root` will be broadcasted to all other workers. source # Lux.DistributedUtils.reduce! — Function . julia\n```julia reduce! (backend :: AbstractLuxDistributedBackend , sendrecvbuf, op; root :: Int = 0 ) \n reduce! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf, op; root :: Int = 0 )``` Backend Agnostic API to perform a reduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf` .\n`op` allows a special `DistributedUtils.avg` operation that averages the result across all workers. source # Lux.DistributedUtils.synchronize!! — Function . julia ```julia synchronize!! (backend :: AbstractLuxDistributedBackend , ps; root :: Int = 0 )```","4. Backend Agnostic API to broadcast the given buffer `sendrecvbuf` or `sendbuf` to all workers into `recvbuf` . The value at `root` will be broadcasted to all other workers. source # Lux.DistributedUtils.reduce! — Function . julia\n```julia reduce! (backend :: AbstractLuxDistributedBackend , sendrecvbuf, op; root :: Int = 0 ) \n reduce! (backend :: AbstractLuxDistributedBackend , sendbuf, recvbuf, op; root :: Int = 0 )``` Backend Agnostic API to perform a reduce operation on the given buffer `sendrecvbuf` or `sendbuf` and store the result in `recvbuf` .\n`op` allows a special `DistributedUtils.avg` operation that averages the result across all workers. source # Lux.DistributedUtils.synchronize!! — Function . julia ```julia synchronize!! (backend :: AbstractLuxDistributedBackend , ps; root :: Int = 0 )```","5. Info Starting from `v0.5` , Lux has transitioned to a new GPU management system. The old system using `cpu` and `gpu` functions is still in place but will be removed in `v1` . Using the old functions might lead to performance regressions if used inside performance critical code. `Lux.jl` can handle multiple GPU backends. Currently, the following backends are supported:\njulia ```julia # Important to load trigger packages \n using  Lux, LuxCUDA  #, AMDGPU, Metal, oneAPI \n \n supported_gpu_backends ()``` ```julia (\"CUDA\", \"AMDGPU\", \"Metal\", \"oneAPI\")``` Metal Support Support for Metal GPUs should be considered extremely experimental at this point."]},{"average_distance":0.3954545454545454,"minimum_distance":0.33585858585858586,"question":"How can scientists from different fields utilize high-performance scientific machine learning methods?","source":"https://sciml.ai/news/2020/03/29/SciML/ - The Software that SciML Provides/We provide high-level domain-specific modeling tools to make scientific modeling more accessible","distance_scores":[0.33585858585858586,0.33585858585858586,0.43434343434343436,0.43434343434343436,0.43686868686868685],"context":"Differential equations appear in nearly every scientific domain, but most scientific domains have their own specialized idioms and terminology. A physicist, biologist, chemist, etc. should be able to pick up our tools and make use of high performance scientific machine learning methods without requiring the understanding of every component and using abstractions that make sense to their field.","answer":"Scientists like physicists, biologists, and chemists can utilize high-performance scientific machine learning methods using adapted tools that do not require understanding every component and use abstractions relevant to their specific fields.","retrieved_contexts":["1. Computational scientific discovery is at an interesting juncture. While we have mechanistic models of lots of different scientific phenomena, and reams of data being generated from experiments - our computational capabilities are unable to keep up. Our problems are too large for realistic simulation. Our problems are multiscale and too stiff.\nOur problems require tedious work like calculating gradients and getting code to run on GPUs and supercomputers. Our next step forward is a combination of science and machine learning, which combines mechanistic models with data based reasoning, presented as a unified set of abstractions and a high performance implementation. We refer to this as scientific machine learning.\nScientific Machine Learning, abbreviated SciML, has been taking the academic world by storm as an interesting blend of traditional scientific mechanistic modeling (differential equations) with machine learning methodologies like deep learning.","2. Computational scientific discovery is at an interesting juncture. While we have mechanistic models of lots of different scientific phenomena, and reams of data being generated from experiments - our computational capabilities are unable to keep up. Our problems are too large for realistic simulation. Our problems are multiscale and too stiff.\nOur problems require tedious work like calculating gradients and getting code to run on GPUs and supercomputers. Our next step forward is a combination of science and machine learning, which combines mechanistic models with data based reasoning, presented as a unified set of abstractions and a high performance implementation. We refer to this as scientific machine learning.\nScientific Machine Learning, abbreviated SciML, has been taking the academic world by storm as an interesting blend of traditional scientific mechanistic modeling (differential equations) with machine learning methodologies like deep learning.","3. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to  automate the discovery of missing physical laws using neural networks embedded in differentiable simulations ? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\nIn this plot, `SciPy` in yellow represents Python's most commonly used solvers:","4. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to  automate the discovery of missing physical laws using neural networks embedded in differentiable simulations ? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\nIn this plot, `MATLAB` in orange represents MATLAB's most commonly used solvers:","5. Multithreading is automatic and baked into many libraries, with a specialized algorithm to ensure hierarchical usage does not oversubscribe threads. Basically, libraries give you a lot of parallelism for free, and doing the rest is a piece of cake.\nMix Scientific Computing with Machine Learning - Want to  automate the discovery of missing physical laws using neural networks embedded in differentiable simulations ? Julia's SciML is the ecosystem with the tooling to integrate machine learning into the traditional high-performance scientific computing domains, from multiphysics simulations to partial differential equations.\nIn this plot, `deSolve` in blue represents R's most commonly used solver:"]},{"average_distance":0.2360655737704918,"minimum_distance":0.0,"question":"Which high-level tools are mentioned as defining pre-made implicit deep learning architectures?","source":"https://sciml.ai/ - SciML: Open Source Software for Scientific Machine Learning/ML-Assisted Tooling for Model Acceleration","distance_scores":[0.3114754098360656,0.23770491803278693,0.24590163934426235,0.0,0.38524590163934425],"context":"High level tools like  DiffEqFlux.jl and  DeepEquilibriumNetworks.jl define pre-made implicit deep learning architectures.","answer":"High level tools like DiffEqFlux.jl and DeepEquilibriumNetworks.jl are mentioned as defining pre-made implicit deep learning architectures.","retrieved_contexts":["1. Implicit layer deep learning is a field which uses implicit rules, such as differential equations and nonlinear solvers, to define the layers of neural networks. This field has brought the potential to automatically optimize network depth and improve training performance.\nSciML's differentiable solver ecosystem is specifically designed to accommodate implicit layer methodologies, and provides libraries with pre-built layers for common methods.","2. While traditional deep learning methodologies have had difficulties with scientific issues like stiffness, interpretability, and enforcing physical constraints, this blend with numerical analysis and differential equations has evolved into a field of research with new methods, architectures, and algorithms which overcome these problems while adding the data-driven automatic\nlearning features of modern deep learning.\nMany successes have already been found, with tools like  physics-informed neural networks ,  universal differential equations ,  deep BSDE solvers for high dimensional PDEs , and  neural surrogates showcasing how deep learning can greatly improve scientific modeling practice.","3. While traditional deep learning methodologies have had difficulties with scientific issues like stiffness, interpretability, and enforcing physical constraints, this blend with numerical analysis and differential equations has evolved into a field of research with new methods, architectures, and algorithms which overcome these problems while adding the data-driven automatic\nlearning features of modern deep learning.\nMany successes have already been found, with tools like  physics-informed neural networks ,  deep BSDE solvers for high dimensional PDEs , and  neural surrogates showcasing how deep learning can greatly improve scientific modeling practice.","4. SciML supports the development of the latest ML-accelerated toolsets for scientific machine learning. Methods like Physics-Informed Neural Networks (PINNs) are productionized in the  NeuralPDE.jl library, while the Deep BSDE, the Deep Splitting and the MLP methods for solving 1000 dimensional partial differential equations are available in the  HighDimPDE.jl library.\nSurrogate-based acceleration methods are provided by  Surrogates.jl . High level tools like  DiffEqFlux.jl and  DeepEquilibriumNetworks.jl define pre-made implicit deep learning architectures.","5. `Boltz.Layers.ClassTokens` `Boltz.Layers.HamiltonianNN` `Boltz.Layers.SplineLayer` `Boltz.Layers.ViPosEmbedding` `Boltz.Basis.Chebyshev` `Boltz.Basis.Cos` `Boltz.Basis.Fourier` `Boltz.Basis.Legendre` `Boltz.Basis.Polynomial` `Boltz.Basis.Sin` `Boltz.Layers.ConvBatchNormActivation` `Boltz.Layers.ConvNormActivation`\n`Boltz.Layers.MLP` `Boltz.Layers.MultiHeadSelfAttention` `Boltz.Layers.TensorProductLayer` `Boltz.Layers.VisionTransformerEncoder` `Boltz.Vision.AlexNet` `Boltz.Vision.ConvMixer` `Boltz.Vision.DenseNet` `Boltz.Vision.GoogLeNet` `Boltz.Vision.MobileNet` `Boltz.Vision.ResNeXt` `Boltz.Vision.ResNet` `Boltz.Vision.VGG`\n`Boltz.Vision.VisionTransformer` Edit this page on GitHub Pager Previous page  Activation Functions Next page  Computer Vision"]},{"average_distance":0.364406779661017,"minimum_distance":0.0,"question":"How does DiffEqSensitivity.jl handle chaotic dynamical systems?","source":"https://sciml.ai/news/2021/08/26/expansion/ - SciML Ecosystem Update: Expansion of the Common Interface/Forward and adjoint shadow sensitivities","distance_scores":[0.0,0.43644067796610164,0.5635593220338984,0.3813559322033898,0.44067796610169496],"context":"The DiffEqSensitivity.jl derivative overloads are now able to handle chaotic dynamical systems via forward and adjoint sensitivity analysis methods. For more information on these techniques, see  Frank's blog post on their development .","answer":"DiffEqSensitivity.jl handles chaotic dynamical systems using forward and adjoint sensitivity analysis methods.","retrieved_contexts":["1. The DiffEqSensitivity.jl derivative overloads are now able to handle chaotic dynamical systems via forward and adjoint sensitivity analysis methods. For more information on these techniques, see  Frank's blog post on their development .","2. DynamicalSystems.jl is an entire ecosystem of dynamical systems analysis methods, for computing measures of chaos (dimension estimation, Lyapunov coefficients), generating delay embeddings, and much more. It uses the SciML tools for its internal equation solving and thus shares much of its API, adding a layer of new tools for extended analyses.\nFor more information, watch the  tutorial Introduction to DynamicalSystems.jl .","3. If you want to learn more about analysing dynamical systems, including chaotic behaviour, see the textbook  Nonlinear Dynamics . It utilizes DynamicalSystems.jl and provides a concise, hands-on approach to learning nonlinear dynamics and analysing dynamical systems   [3] .","4. SteadyStateDiffEq.jl uses ODE solvers to iteratively approach the steady state. It is a very stable method for solving nonlinear systems, though often computationally more expensive than direct methods.\n`DynamicSS` : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state. `DynamicSS(alg; tspan = Inf)` requires that an ODE algorithm is given as the first argument. The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero.","5. SteadyStateDiffEq.jl uses ODE solvers to iteratively approach the steady state. It is a very stable method for solving nonlinear systems with stable equilibrium points, though often more computationally expensive than direct methods. `DynamicSS()` : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state.\n`SSRootfind()` : Uses a NonlinearSolve compatible solver to find the steady state."]},{"average_distance":0.31380145278450366,"minimum_distance":0.002421307506053294,"question":"What does structural identifiability analysis demonstrate regarding parameter interactions, and how is this analysis conducted?","source":"https://sciml.ai/gsoc/gsoc_symbolic - SciML Symbolic-Numeric Computing Projects – Google Summer of Code/Parameter identifiability analysis","distance_scores":[0.002421307506053294,0.46731234866828086,0.23728813559322037,0.3946731234866828,0.46731234866828086],"context":"Structural identifiability analysis relates changes in the solution of the ODE directly to other parameters, showcasing that it is impossible to distinguish between parameter A being higher and parameter B being lower, or the vice versa situation, given only data about the solution because of how the two interact. This could be done directly on the symbolic form of the equation as part of  ModelingToolkit.jl .","answer":"Structural identifiability analysis shows that it is impossible to distinguish between parameter A being higher and parameter B being lower, or vice versa, based solely on the solution data due to their interaction. This analysis is conducted directly on the symbolic form of the equation as part of ModelingToolkit.jl.","retrieved_contexts":["1. Parameter identifiability analysis is an analysis that describes whether the parameters of a dynamical system can be identified from data or whether they are redundant. There are two forms of identifiability analysis: structural and practical.\nStructural identifiability analysis relates changes in the solution of the ODE directly to other parameters, showcasing that it is impossible to distinguish between parameter A being higher and parameter B being lower, or the vice versa situation, given only data about the solution because of how the two interact.\nThis could be done directly on the symbolic form of the equation as part of  ModelingToolkit.jl . Meanwhile, practical identifiability analysis looks as to whether the parameters are non-identifiable in a practical sense, for example if two parameters are numerically indistinguishable (given possibly noisy data).","2. Can we recognize which formulations are hard and automatically transform them into the easy ones? Yes.\nStructural parameter identifiability. When fitting parameters to data, there's always assumptions about whether there is a unique parameter set that achieves such a data fit.\nBut is this actually the case? The structural identifiability tooling allows one to analytically determine whether, in the limit of infinite data on a subset of observables, one could in theory uniquely identify the parameters (global identifiability), identify the parameters up to a discrete set (local identifiability), or whether there's an infinite manifold of solutions to the","3. Parameter identifiability analysis is an analysis that describes whether the parameters of a dynamical system can be identified from data or whether they are redundant. There are two forms of identifiability analysis: structural and practical.\nStructural identifiability analysis relates changes in the solution of the ODE directly to other parameters, showcasing that it is impossible to distinguish between parameter A being higher and parameter B being lower, or the vice versa situation, given only data about the solution because of how the two interact.","4. During parameter fitting, parameter values are inferred from data. Parameter identifiability refers to whether inferring parameter values for a given model is mathematically feasible. Ideally, parameter fitting should always be accompanied with an identifiability analysis of the problem.\nIdentifiability can be divided into  structural and  practical identifiability   [1] . Structural identifiability considers only the mathematical model, and which parameters are and are not inherently identifiable due to model structure. Practical identifiability also considers the available data, and determines what system quantities can be inferred from it.\nIn the idealised case of an infinite amount of non-noisy data, practical identifiability converges to structural identifiability. Generally, structural identifiability is assessed before parameters are fitted, while practical identifiability is assessed afterwards.","5. Structural parameter identifiability. When fitting parameters to data, there's always assumptions about whether there is a unique parameter set that achieves such a data fit.\nBut is this actually the case? The structural identifiability tooling allows one to analytically determine whether, in the limit of infinite data on a subset of observables, one could in theory uniquely identify the parameters (global identifiability), identify the parameters up to a discrete set (local identifiability), or whether there's an infinite manifold of solutions to the"]},{"average_distance":0.4203190967108493,"minimum_distance":0.0020618556701030855,"question":"Why is a much longer chain usually required for Metropolis-Hastings (MH) sampling to converge to an appropriate parameter estimate?","source":"https://turinglang.org/docs/tutorials/docs-15-using-turing-sampler-viz/index.html - Sampler Visualization/Samplers/MH","distance_scores":[0.0020618556701030855,0.5711340206185567,0.5381443298969072,0.4783505154639175,0.5119047619047619],"context":"Metropolis-Hastings (MH) sampling is one of the earliest Markov Chain Monte Carlo methods. MH sampling does not “move” a lot, unlike many of the other samplers implemented in Turing. Typically a much longer chain is required to converge to an appropriate parameter estimate. The plot below only uses 1,000 iterations of Metropolis-Hastings. ```julia c  =   sample (model,  MH (),  1000 ) \n plot_sampler (c)``` As you can see, the MH sampler doesn’t move parameter estimates very often.","answer":"Metropolis-Hastings (MH) sampling is one of the earliest Markov Chain Monte Carlo methods. MH sampling does not “move” a lot, meaning it doesn’t change parameter estimates frequently. Therefore, a much longer chain is required for the estimates to converge to appropriate values.","retrieved_contexts":["1. Metropolis-Hastings (MH) sampling is one of the earliest Markov Chain Monte Carlo methods. MH sampling does not “move” a lot, unlike many of the other samplers implemented in Turing. Typically a much longer chain is required to converge to an appropriate parameter estimate. The plot below only uses 1,000 iterations of Metropolis-Hastings.\n```julia c  =   sample (model,  MH (),  1000 ) \n plot_sampler (c)``` As you can see, the MH sampler doesn’t move parameter estimates very often.","2. Hamiltonian Monte Carlo (HMC) sampling is a typical sampler to use, as it tends to be fairly good at converging in a efficient manner. It can often be tricky to set the correct parameters for this sampler however, and the `NUTS` sampler is often easier to run if you don’t want to spend too much time fiddling with step size and and the number of steps to take.\nNote however that `HMC` does not explore the positive values μ very well, likely due to the leapfrog and step size parameter settings.","3. We use the Gelman, Rubin, and Brooks Diagnostic to check whether our chains have converged. Note that we require multiple chains to use this diagnostic which analyses the difference between these multiple chains.\nWe expect the chains to have converged. This is because we have taken sufficient number of iterations (1500) for the NUTS sampler. However, in case the test fails, then we will have to take a larger number of iterations, resulting in longer computation time. ```julia gelmandiag (chain)```\n```julia Gelman, Rubin, and Brooks diagnostic\n  parameters      psrf    psrfci\n      Symbol   Float64   Float64\n\n          b0    1.0606    1.0913\n          b1    1.0213    1.0410\n          b2    1.1241    1.1966\n          b3    1.1954    1.3484```","4. After we’ve done that tidying, it’s time to split our dataset into training and testing sets, and separate the features and target from the data. Additionally, we must rescale our feature variables so that they are centered around zero by subtracting each column by the mean and dividing it by the standard deviation.\nWithout this step, Turing’s sampler will have a hard time finding a place to start searching for parameter estimates.\n```julia # Split our dataset 50%/50% into training/test sets. \n trainset, testset  =   splitobs ( shuffleobs (data),  0.5 ) \n \n # Define features and target. \n features  =  [ : SepalLength,  : SepalWidth,  : PetalLength,  : PetalWidth] \n target  =   : Species_index \n \n # Turing requires data in matrix and vector form.","5. Metropolis-Hastings is often the first sampling method that people are exposed to. It is a very straightforward algorithm and is accordingly the easiest to implement, so it makes for a good example. In this section, you will learn how to use the types and functions listed above to implement the Metropolis-Hastings sampler using the MCMC interface.\nThe full code for this implementation is housed in  AdvancedMH.jl ."]},{"average_distance":0.4111625216888374,"minimum_distance":0.002024291497975672,"question":"How does Turing.jl handle progress logging during sampling, and what are the options to control it?","source":"https://turinglang.org/docs/tutorials/docs-12-using-turing-guide/index.html - Guide/Beyond the Basics/Changing Default Settings","distance_scores":[0.002024291497975672,0.5714285714285714,0.5714285714285714,0.49595141700404854,0.4149797570850202],"context":"For more information on Turing’s automatic differentiation backend, please see the  Automatic Differentiation article. `Turing.jl` uses ProgressLogging.jl to log the sampling progress. Progress logging is enabled as default but might slow down inference. It can be turned on or off by setting the keyword argument `progress` of `sample` to `true` or `false` . Moreover, you can enable or disable progress logging globally by calling `setprogress!(true)` or `setprogress!(false)` , respectively.","answer":"Turing.jl uses ProgressLogging.jl to log the sampling progress. By default, progress logging is enabled, which might slow down inference. It can be controlled during sampling by setting the `progress` keyword argument of `sample` to `true` or `false`. Additionally, progress logging can be enabled or disabled globally by calling `setprogress!(true)` or `setprogress!(false)`, respectively.","retrieved_contexts":["1. For more information on Turing’s automatic differentiation backend, please see the  Automatic Differentiation article.\n`Turing.jl` uses ProgressLogging.jl to log the sampling progress. Progress logging is enabled as default but might slow down inference. It can be turned on or off by setting the keyword argument `progress` of `sample` to `true` or `false` . Moreover, you can enable or disable progress logging globally by calling `setprogress!(true)` or `setprogress!(false)` , respectively.\nTuring uses heuristics to select an appropriate visualization backend. If you use Jupyter notebooks, the default backend is  ConsoleProgressMonitor.jl . In all other cases, progress logs are displayed with  TerminalLoggers.jl . Alternatively, if you provide a custom visualization backend, Turing uses it instead of the default backend. Back to top","2. We will use  Turing.jl with  Lux.jl to implement implementing a classification algorithm. Lets start by importing the relevant libraries. julia ```julia # Import libraries \n \n using  Lux, Turing, CairoMakie, Random, Tracker, Functors, LinearAlgebra \n \n # Sampling progress \n Turing . setprogress! ( true );```\n```julia [ Info: [Turing]: progress logging is enabled globally \n [ Info: [AdvancedVI]: global PROGRESS is set as true```","3. We will use  Turing.jl with  Lux.jl to implement implementing a classification algorithm. Lets start by importing the relevant libraries. julia ```julia # Import libraries \n \n using  Lux, Turing, CairoMakie, Random, Tracker, Functors, LinearAlgebra \n \n # Sampling progress \n Turing . setprogress! ( true );```\n```julia [ Info: [Turing]: progress logging is enabled globally \n [ Info: [AdvancedVI]: global PROGRESS is set as true```","4. Saranjeet Kaur ’s  project focused primarily on expanding  NestedSamplers.jl . NestedSamplers.jl now supports  PolyChord-style nested sampling natively, which is an absolute delight. Saranjeet wrote about this  here . She also provided a good tutorial on how to use NestedSamplers.jl  here .\nThe NestedSamplers.jl integration with Turing is still on-going – integrating new samplers with Turing is one of the more difficult tasks. If you are interested to see the progress on this, check out the relevant  pull request .\nArthur Lui ’s  project was to provide a much-needed set of benchmarks of Bayesian nonparametric models between Turing and other PPLs. Arthur’s work spawned a  GitHub repository with good practices for benchmarking, as well as three blog posts with some (very cool!) statistics on Turing’s performance:","5. `wrap` : Toggles whether to wrap the solution if `prob.problem_type` has a preferred alternate wrapper type for the solution. Useful when speed, but not shape of solution is important. Defaults to `Val(true)` . `Val(false)` will cancel wrapping the solution. Progress Monitoring\nThese arguments control the usage of the progressbar in ProgressLogging.jl compatible environments. `progress` : Turns on/off the Juno progressbar. Default is false. `progress_steps` : Numbers of steps between updates of the progress bar. Default is 1000. `progress_name` : Controls the name of the progressbar. Default is the name of the problem type.\n`progress_message` : Controls the message with the progressbar. Defaults to showing `dt` , `t` , the maximum of `u` . `progress_id` : Controls the ID of the progress log message to distinguish simultaneous simulations. Error Calculations If you are using the test problems (ex: `ODETestProblem` ), then the following options control the errors which are calculated:"]},{"average_distance":0.3531420204079706,"minimum_distance":0.00421940928270037,"question":"How does the Gaussian Process Latent Variable Model (GPLVM) extend the mapping provided by Probabilistic Principal Component Analysis (pPCA)?","source":"https://turinglang.org/docs/tutorials/12-gplvm/index.html - Gaussian Process Latent Variable Model","distance_scores":[0.17510548523206748,0.5763888888888888,0.00421940928270037,0.5458612975391499,0.4641350210970464],"context":"Documentation   Using Turing - Tutorials   Gaussian Process Latent Variable Models In a previous tutorial, we have discussed latent variable models, in particular probabilistic principal component analysis (pPCA). Here, we show how we can extend the mapping provided by pPCA to non-linear mappings between input and output. For more details about the Gaussian Process Latent Variable Model (GPLVM), we refer the reader to the  original publication and a  further extension .","answer":"The Gaussian Process Latent Variable Model (GPLVM) extends the mapping provided by Probabilistic Principal Component Analysis (pPCA) to non-linear mappings between input and output.","retrieved_contexts":["1. In a previous tutorial, we have discussed latent variable models, in particular probabilistic principal component analysis (pPCA). Here, we show how we can extend the mapping provided by pPCA to non-linear mappings between input and output.\nFor more details about the Gaussian Process Latent Variable Model (GPLVM), we refer the reader to the  original publication and a  further extension .\nIn short, the GPVLM is a dimensionality reduction technique that allows us to embed a high-dimensional dataset in a lower-dimensional embedding. Importantly, it provides the advantage that the linear mappings from the embedded space can be non-linearised through the use of Gaussian Processes.","2. transform! (dt, dat);```\nWe will start out by demonstrating the basic similarity between pPCA (see the tutorial on this topic) and the GPLVM model. Indeed, pPCA is basically equivalent to running the GPLVM model with an automatic relevance determination (ARD) linear kernel. First, we re-introduce the pPCA model (see the tutorial on pPCA for details)\n```julia @model   function   pPCA (x) \n      # Dimensionality of the problem.","3. Documentation   Using Turing - Tutorials   Gaussian Process Latent Variable Models\nIn a previous tutorial, we have discussed latent variable models, in particular probabilistic principal component analysis (pPCA). Here, we show how we can extend the mapping provided by pPCA to non-linear mappings between input and output.\nFor more details about the Gaussian Process Latent Variable Model (GPLVM), we refer the reader to the  original publication and a  further extension .","4. For more details about the Gaussian Process Latent Variable Model (GPLVM), we refer the reader to the  original publication and a  further extension .\nIn short, the GPVLM is a dimensionality reduction technique that allows us to embed a high-dimensional dataset in a lower-dimensional embedding. Importantly, it provides the advantage that the linear mappings from the embedded space can be non-linearised through the use of Gaussian Processes.","5. where  \\(\\mathbf{U}\\) and  \\(\\mathbf{V}\\) are unitary matrices (i.e. with orthonormal columns). Such a decomposition always exists for any matrix. Columns of  \\(\\mathbf{V}\\) are the principal directions/axes. The percentage of variations explained can be calculated using the ratios of singular values.   2\nHere we take a probabilistic perspective. For more details and a mathematical derivation, we recommend Bishop’s textbook (Christopher M. Bishop, Pattern Recognition and Machine Learning, 2006). The idea of proabilistic PCA is to find a latent variable  \\(z\\) that can be used to describe the hidden structure in a dataset.\n3 Consider a data set  \\(\\mathbf{X}_{D \\times N}=\\{x_i\\}\\) with  \\(i=1,2,...,N\\) data points, where each data point  \\(x_i\\) is  \\(D\\) -dimensional (i.e.  \\(x_i \\in \\mathcal{R}^D\\) ). Note that, here we use the flipped version of the data matrix."]},{"average_distance":0.3888372093023256,"minimum_distance":0.0023255813953488857,"question":"When should density calculations ignore the volume element of variate transformations?","source":"https://juliamath.github.io/MeasureTheory.jl/stable/api_measurebase - MeasureBase API","distance_scores":[0.0023255813953488857,0.40930232558139534,0.6023255813953489,0.5116279069767442,0.41860465116279066],"context":"Indicate that density calculations should ignore the volume element of variate transformations. Should only be used in special cases in which the volume element has already been taken into account in a different way. ```julia struct PowerMeasure{M,...} <: AbstractProductMeasure``` A power measure is a product of a measure with itself. The number of elements in the product determines the dimensionality of the resulting support.","answer":"Density calculations should ignore the volume element of variate transformations only in special cases where the volume element has already been accounted for in a different manner.","retrieved_contexts":["1. ```julia struct MeasureBase.NoTransportOrigin{NU}``` Indicates that no (default) pullback measure is available for measures of type `NU` . See  `MeasureBase.transport_origin` . ```julia NoVolCorr()```\nIndicate that density calculations should ignore the volume element of variate transformations. Should only be used in special cases in which the volume element has already been taken into account in a different way. ```julia struct PowerMeasure{M,...} <: AbstractProductMeasure```\nA power measure is a product of a measure with itself. The number of elements in the product determines the dimensionality of the resulting support. Note that power measures are only well-defined for integer powers. The nth power of a measure μ can be written μ^n. ```julia abstract type PrimitiveMeasure <: AbstractMeasure end```","2. ```julia abstract type TransformVolCorr``` Provides control over density correction by transform volume element. Either  `NoVolCorr()` or  `WithVolCorr()` ```julia struct TransportFunction <: Function``` Transforms a variate from one measure to a variate of another. In general `TransportFunction` should not be called directly, call  `transport_to` instead.\n```julia struct UnknownFiniteMass <: AbstractUnknownMass end``` See `massof` ```julia struct UnknownMass <: AbstractUnknownMass end``` See `massof` ```julia WithVolCorr()``` Indicate that density calculations should take the volume element of variate transformations into account (typically via the log-abs-det-Jacobian of the transform).\n```julia (m::AbstractMeasure) | constraint``` Return a new measure by constraining `m` to satisfy `constraint` .","3. If the \"un-transformed\" `z` is univariate, things are relatively simple. But it's important our approach handle the multivariate case as well.\nIn the literature, it's common for a multivariate normal distribution to be parameterized by a mean `μ` and covariance matrix `Σ` . This is mathematically convenient, but leads to an  $O(n^3)$  Cholesky decomposition , which becomes a significant bottleneck to compute as  $n$ gets large.","4. `AlgebraOfGraphics.jl` can perform  statistical transformations as layers with five functions: `expectation` : calculates the mean (  expectation ) of the underlying Y-axis column `frequency` : computes the frequency (  raw count ) of the underlying X-axis column `density` : computes the density (  distribution ) of the underlying X-axis column\n`linear` : computes a linear trend relationship between the underlying X- and Y-axis columns `smooth` : computes a smooth relationship between the underlying X- and Y-axis columns Let’s first cover `expectation` : ```julia plt = data(df) *\n    mapping(:name, :grade) *\n    expectation()\ndraw(plt)``` Figure 55: AlgebraOfGraphics bar plot with expectation.","5. & =\\frac{f}{1+\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\alpha}}+\\frac{g}{\\frac{\\mathrm{d}\\alpha}{\\mathrm{d}\\beta}+1}\\\\\n     & =\\frac{f}{1+\\left(\\frac{\\mathrm{d}\\alpha}{\\mathrm{d}\\beta}\\right)^{-1}}+\\frac{g}{\\frac{\\mathrm{d}\\alpha}{\\mathrm{d}\\beta}+1}\\ .\n    \\end{aligned}\\]\n```julia abstract type TransformVolCorr``` Provides control over density correction by transform volume element. Either  `NoVolCorr()` or  `WithVolCorr()` ```julia struct TransportFunction <: Function``` Transforms a variate from one measure to a variate of another. In general `TransportFunction` should not be called directly, call  `transport_to` instead.\n```julia struct UnknownFiniteMass <: AbstractUnknownMass end``` See `massof` ```julia struct UnknownMass <: AbstractUnknownMass end``` See `massof` ```julia WithVolCorr()``` Indicate that density calculations should take the volume element of variate transformations into account (typically via the log-abs-det-Jacobian of the transform)."]},{"average_distance":0.33646515976402874,"minimum_distance":0.0022831050228310223,"question":"What makes Turing sampling methods compatible with models defined by the `@model` macro?","source":"https://turinglang.org/docs/tutorials/docs-04-for-developers-abstractmcmc-turing/index.html - How Turing implements AbstractMCMC/Introduction","distance_scores":[0.0022831050228310223,0.40410958904109584,0.4908675799086758,0.26190476190476186,0.5231607629427792],"context":"Turing sampling methods (most of which are written  here ) also implement `AbstractMCMC` . Turing defines a particular architecture for `AbstractMCMC` implementations, that enables working with models defined by the `@model` macro, and uses DynamicPPL as a backend. The goal of this page is to describe this architecture, and how you would go about implementing your own sampling method in Turing, using Importance Sampling as an example.","answer":"Turing sampling methods are compatible with models defined by the `@model` macro because they implement `AbstractMCMC` and use DynamicPPL as a backend. Turing defines a specific architecture for `AbstractMCMC` implementations that facilitates this compatibility.","retrieved_contexts":["1. The function `sample` is part of the AbstractMCMC interface. As explained in the  interface guide , building a sampling method that can be used by `sample` consists in overloading the structs and functions in `AbstractMCMC` . The interface guide also gives a standalone example of their implementation,  `AdvancedMH.jl` .\nTuring sampling methods (most of which are written  here ) also implement `AbstractMCMC` . Turing defines a particular architecture for `AbstractMCMC` implementations, that enables working with models defined by the `@model` macro, and uses DynamicPPL as a backend.\nThe goal of this page is to describe this architecture, and how you would go about implementing your own sampling method in Turing, using Importance Sampling as an example. I don’t go into all the details: for instance, I don’t address selectors or parallelism.","2. The following terminology will be used in this section: `D` : observed data variables conditioned upon in the posterior, `P` : parameter variables distributed according to the prior distributions, these will also be referred to as random variables, `Model` : a fully defined probabilistic model with input data\n`Turing` ’s `@model` macro rewrites the user-provided function definition such that it can be used to instantiate a `Model` by passing in the observed data `D` . The following are the main jobs of the `@model` macro: Parse `~` and `.~` lines, e.g. `y .~ Normal.(c*x, 1.0)` Figure out if a variable belongs to the data `D` and or to the parameters `P`\nEnable the handling of missing data variables in `D` when defining a `Model` and treating them as parameter variables in `P` instead Enable the tracking of random variables using the data structures `VarName` and `VarInfo` Change `~` / `.~` lines with a variable in `P` on the LHS to a call to `tilde_assume` or `dot_tilde_assume`","3. We implement our sampler in the  AbstractMCMC.jl interface, which just means that our sampler is a subtype of `AbstractMCMC.AbstractSampler` and we implement the MCMC transition in `AbstractMCMC.step` .\nPoints 1 and 2 makes it so our sampler can be used with a wide range of model implementations, amongst them being models implemented in both Turing.jl and Stan. This gives you, the inference implementer, a large collection of models to test your inference method on, in addition to allowing users of Turing.jl and Stan to try out your inference method with minimal effort.","4. Documentation   Using Turing - Modelling Syntax and Interface   External Samplers","5. Turing implements a sampling interface (hosted at  AbstractMCMC ) that is intended to provide a common framework for Markov chain Monte Carlo samplers. The interface presents several structures and functions that one needs to overload in order to implement an interface-compatible sampler. This guide will demonstrate how to implement the interface without Turing."]}]